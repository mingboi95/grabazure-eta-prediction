{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QznqRj4Zxzqb"
   },
   "source": [
    "# This notebook has 3 parts:\n",
    "- Unioning all the parquet files\n",
    "- Adding in the spatial data\n",
    "- Adding in the weather data\n",
    "\n",
    "## Note: put this notebook in the same folder as:\n",
    "- city=singapore\n",
    "- planning-area-census2010-shp.zip\n",
    "- may_apr_weather.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W7ktwuyhxgHW"
   },
   "source": [
    "### Loading all relevant libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "TPD49UVdxfAQ",
    "outputId": "130017cf-6bc1-476c-d054-14be1f16fccb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopandas in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (0.7.0)\n",
      "Requirement already satisfied: fiona in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from geopandas) (1.8.13.post1)\n",
      "Requirement already satisfied: pandas>=0.23.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from geopandas) (0.23.4)\n",
      "Requirement already satisfied: shapely in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from geopandas) (1.7.0)\n",
      "Requirement already satisfied: pyproj>=2.2.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from geopandas) (2.6.1.post1)\n",
      "Requirement already satisfied: cligj>=0.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from fiona->geopandas) (0.5.0)\n",
      "Requirement already satisfied: six>=1.7 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from fiona->geopandas) (1.12.0)\n",
      "Requirement already satisfied: attrs>=17 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from fiona->geopandas) (19.3.0)\n",
      "Requirement already satisfied: click<8,>=4.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from fiona->geopandas) (7.1.2)\n",
      "Requirement already satisfied: click-plugins>=1.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from fiona->geopandas) (1.1.1)\n",
      "Requirement already satisfied: munch in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from fiona->geopandas) (2.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from pandas>=0.23.0->geopandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2011k in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from pandas>=0.23.0->geopandas) (2019.3)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from pandas>=0.23.0->geopandas) (1.16.2)\n",
      "Requirement already satisfied: pyshp in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (2.1.0)\n",
      "Requirement already satisfied: descartes in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (1.1.0)\n",
      "Requirement already satisfied: matplotlib in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from descartes) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from matplotlib->descartes) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from matplotlib->descartes) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from matplotlib->descartes) (2.4.7)\n",
      "Requirement already satisfied: numpy>=1.11 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from matplotlib->descartes) (1.16.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from matplotlib->descartes) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib->descartes) (1.12.0)\n",
      "Requirement already satisfied: fiona in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (1.8.13.post1)\n",
      "Requirement already satisfied: six>=1.7 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from fiona) (1.12.0)\n",
      "Requirement already satisfied: click-plugins>=1.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from fiona) (1.1.1)\n",
      "Requirement already satisfied: munch in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from fiona) (2.5.0)\n",
      "Requirement already satisfied: attrs>=17 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from fiona) (19.3.0)\n",
      "Requirement already satisfied: click<8,>=4.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from fiona) (7.1.2)\n",
      "Requirement already satisfied: cligj>=0.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from fiona) (0.5.0)\n",
      "Requirement already satisfied: shapely in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (1.7.0)\n",
      "Requirement already satisfied: pyproj in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (2.6.1.post1)\n",
      "Requirement already satisfied: rtree<0.9,>=0.8 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (0.8.3)\n",
      "Requirement already satisfied: setuptools in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from rtree<0.9,>=0.8) (46.4.0.post20200518)\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "python3-rtree is already the newest version (0.8.2+ds-2).\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  cmake-data grub-pc-bin libarchive13 libjsoncpp1\n",
      "Use 'sudo apt autoremove' to remove them.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 27 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!pip install geopandas\n",
    "!pip install pyshp\n",
    "!pip install descartes\n",
    "!pip install fiona\n",
    "!pip install shapely\n",
    "!pip install pyproj\n",
    "!pip install \"rtree>=0.8,<0.9\"\n",
    "!sudo apt install python3-rtree -y\n",
    "\n",
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "from math import radians, cos, sin, asin, sqrt, atan2, pi\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import shapefile\n",
    "from shapely.geometry import shape  \n",
    "import rtree\n",
    "import geopandas as gpd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I8Q4fx7GwqHm"
   },
   "source": [
    "### Unioning all the parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QqgXCqZ2wqPd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting file 1 now\n",
      "Unioning parquet files done\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "def TimeGroup(hour):\n",
    "    if hour <= 7:\n",
    "        return 'late night'\n",
    "    elif hour <= 9:\n",
    "        return 'morning peak'\n",
    "    elif hour <= 18:\n",
    "        return 'day'\n",
    "    elif hour <= 20:\n",
    "        return 'evening peak'\n",
    "    elif hour <= 23:\n",
    "        return 'night'\n",
    "    \n",
    "def Duration(series):\n",
    "    return max(series) - min(series)\n",
    "\n",
    "def Haversine(lon1, lat1, lon2, lat2):\n",
    "\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371e3\n",
    "    return c * r\n",
    "\n",
    "def GetDistance(orilat, orilng, deslat, deslng):\n",
    "    result = []\n",
    "    for i in range(len(orilat)):\n",
    "        result.append( Haversine(orilng.iloc[i], orilat.iloc[i], deslng.iloc[i], deslat.iloc[i]) )\n",
    "    return pd.Series(result)\n",
    "\n",
    "def GetDirection(orilat, orilng, deslat, deslng):\n",
    "    result = []\n",
    "    for i in range(len(orilat)):\n",
    "        \n",
    "        lat1, lon1, lat2, lon2 = orilat.iloc[i], orilng.iloc[i], deslat.iloc[i], deslng.iloc[i]\n",
    "        dlon = lon2 - lon1 \n",
    "        \n",
    "        X = cos(lat2) * sin(dlon)\n",
    "        Y = cos(lat1) * sin(lat2) - sin(lat1) * cos(lat2) * cos(dlon)\n",
    "        \n",
    "        result.append( ( (atan2(X,Y)*180/pi) + 360) % 360 )\n",
    "        \n",
    "    return pd.Series(result)\n",
    "\n",
    "\n",
    "# Getting the data, but remove the 'break' to use full data, if not just 1 of the files\n",
    "counter = 1\n",
    "for file in os.listdir('city=singapore'):\n",
    "    print('Getting file {} now'.format(counter))\n",
    "    DF = pq.read_table('city=singapore/' + file).to_pandas()\n",
    "    \n",
    "    df = df.append(DF, ignore_index=True)\n",
    "    counter += 1\n",
    "\n",
    "id_col = ['trj_id']\n",
    "numerical_cols = ['rawlat', 'rawlng', 'speed', 'bearing', 'accuracy']\n",
    "categorical_cols = ['driving_mode', 'osname']\n",
    "\n",
    "#since there is only one unique value under driving_mode we will drop this column\n",
    "del df['driving_mode']\n",
    "\n",
    "# sorting according to date\n",
    "df.sort_values('pingtimestamp', axis=0, inplace=True, kind='mergesort')\n",
    "\n",
    "# Creating new columns\n",
    "df['avg_speed'] = df.speed\n",
    "df['hour'] = df.pingtimestamp\n",
    "df['day of week'] = df.pingtimestamp\n",
    "df['day'] = df.pingtimestamp\n",
    "df['month'] = df.pingtimestamp\n",
    "df['is_Weekday'] = df.pingtimestamp\n",
    "df['time_group'] = df.pingtimestamp\n",
    "df['origin_lat'] = df.rawlat\n",
    "df['origin_lng'] = df.rawlng\n",
    "df['dest_lat'] = df.rawlat\n",
    "df['dest_lng'] = df.rawlng\n",
    "df['duration'] = df.pingtimestamp\n",
    "\n",
    "# Aggregation of the new columns\n",
    "df = df.groupby('trj_id', as_index=False).agg({'avg_speed': 'mean',\n",
    "                                               'osname': 'first',\n",
    "                                               'hour': lambda x: pd.to_datetime(min(x), unit='s').hour,\n",
    "                                               'day of week': lambda x: pd.to_datetime(min(x), unit='s').dayofweek,\n",
    "                                               'day': lambda x: pd.to_datetime(min(x), unit='s').day,\n",
    "                                               'month': lambda x: pd.to_datetime(min(x), unit='s').month,\n",
    "                                               'is_Weekday': lambda x: 1 if pd.to_datetime(min(x), unit='s').weekday() < 5 else 0,\n",
    "                                               'time_group': lambda x: TimeGroup(pd.to_datetime(min(x), unit='s').hour),\n",
    "                                               'origin_lat': 'first',\n",
    "                                               'origin_lng': 'first',\n",
    "                                               'dest_lat': 'last',\n",
    "                                               'dest_lng': 'last',\n",
    "                                               'duration': lambda x: Duration(x)\n",
    "                                               })\n",
    "\n",
    "df['euclid_dist'] = GetDistance(df.origin_lat, df.origin_lng, df.dest_lat, df.dest_lng)\n",
    "\n",
    "df['avg_bearing'] = GetDirection(df.origin_lat, df.origin_lng, df.dest_lat, df.dest_lng)\n",
    "\n",
    "print('Unioning parquet files done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EfWTuCUpa9Co"
   },
   "source": [
    "### Adding in the spatial data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "s8ReYv8sgZEj",
    "outputId": "65e799b8-8617-4876-f860-7dbc7ac921fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/geopandas/tools/sjoin.py:61: UserWarning: CRS of frames being joined does not match!(None != epsg:4326)\n",
      "  \"(%s != %s)\" % (left_df.crs, right_df.crs)\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/geopandas/tools/sjoin.py:61: UserWarning: CRS of frames being joined does not match!(None != epsg:4326)\n",
      "  \"(%s != %s)\" % (left_df.crs, right_df.crs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding spatial data done\n"
     ]
    }
   ],
   "source": [
    "# df get from previous code on top ^^\n",
    "# df = pd.read_csv('all.csv')\n",
    "\n",
    "zipfile = ZipFile('planning-area-census2010-shp.zip')\n",
    "filenames = [y for y in sorted(zipfile.namelist()) for ending in ['dbf', 'prj', 'shp', 'shx'] if y.endswith(ending)] \n",
    "\n",
    "dbf, prj, shp, shx = [BytesIO(zipfile.read(filename)) for filename in filenames]\n",
    "r = shapefile.Reader(shp=shp, shx=shx, dbf=dbf)\n",
    "\n",
    "attributes, geometry = [], []\n",
    "field_names = [field[0] for field in r.fields[1:]]  \n",
    "for row in r.shapeRecords():  \n",
    "    geometry.append(shape(row.shape.__geo_interface__))  \n",
    "    attributes.append(dict(zip(field_names, row.record)))\n",
    "\n",
    "gdf = gpd.GeoDataFrame(data = attributes, geometry = geometry, crs = 'epsg:3414')\n",
    "gdf.geometry = gdf.geometry.to_crs(epsg=4326)\n",
    "\n",
    "points = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.origin_lng, df.origin_lat))\n",
    "\n",
    "complete = gpd.sjoin(points, gdf, op = \"within\")\n",
    "\n",
    "df_complete = pd.DataFrame(complete)\n",
    "\n",
    "df_complete = df_complete[['trj_id', \n",
    "                           'avg_speed', \n",
    "                           'avg_bearing', \n",
    "                           'osname',\n",
    "                           'hour',\n",
    "                           'day',\n",
    "                           'month',\n",
    "                           'is_Weekday',\n",
    "                           'time_group',\n",
    "                           'origin_lat',\n",
    "                           'origin_lng',\n",
    "                           'dest_lat',\n",
    "                           'dest_lng',\n",
    "                           'duration',\n",
    "                           'euclid_dist',\n",
    "                           'PLN_AREA_N',\n",
    "                           'REGION_N']]\n",
    "\n",
    "df_complete['origin_subregion'] = df_complete['PLN_AREA_N']\n",
    "df_complete['origin_region'] = df_complete['REGION_N']\n",
    "del df_complete['PLN_AREA_N']\n",
    "del df_complete['REGION_N']\n",
    "\n",
    "points_dest = gpd.GeoDataFrame(df_complete, geometry=gpd.points_from_xy(df_complete.dest_lng, df_complete.dest_lat))\n",
    "\n",
    "another = gpd.sjoin(points_dest, gdf, op = 'within')\n",
    "df_updated = pd.DataFrame(another)\n",
    "\n",
    "df_updated = df_updated[['trj_id', \n",
    "                         'avg_speed', \n",
    "                         'avg_bearing', \n",
    "                         'osname', \n",
    "                         'hour',\n",
    "                         'day', \n",
    "                         'month',\n",
    "                         'is_Weekday',\n",
    "                         'time_group', \n",
    "                         'origin_lat', \n",
    "                         'origin_lng', \n",
    "                         'dest_lat', \n",
    "                         'dest_lng', \n",
    "                         'duration', \n",
    "                         'euclid_dist', \n",
    "                         'origin_subregion', \n",
    "                         'origin_region',\n",
    "                         'PLN_AREA_N', \n",
    "                         'REGION_N']]\n",
    "\n",
    "df_updated['dest_subregion'] = df_updated['PLN_AREA_N']\n",
    "df_updated['dest_region'] = df_updated['REGION_N']\n",
    "del df_updated['PLN_AREA_N']\n",
    "del df_updated['REGION_N']\n",
    "\n",
    "# writing to csv\n",
    "# df_updated.to_csv('all_updated.csv')\n",
    "\n",
    "df = df_updated\n",
    "\n",
    "print('Adding spatial data done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VgTfhuz8KT-r"
   },
   "source": [
    "### Adding in the weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JMZTPgR_KWGb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding weather data done\n"
     ]
    }
   ],
   "source": [
    "weather = pd.read_csv('may_apr_weather.csv')\n",
    "\n",
    "# df get from the previous code on top ^^\n",
    "# df = pd.read_csv('all_updated.csv')\n",
    "\n",
    "# dummy column for origin subregion for joining, need this because our dataset has 53 subregions but weather only 38\n",
    "df['dummy_origin'] = df['origin_subregion']\n",
    "conditions = [\n",
    "    (df['dummy_origin'] == 'BUKIT BATOK'),\n",
    "    (df['dummy_origin'] == 'HOUGANG'),\n",
    "    (df['dummy_origin'] == 'JURONG EAST'),\n",
    "    (df['dummy_origin'] == 'MARINA EAST'),\n",
    "    (df['dummy_origin'] == 'OUTRAM'),\n",
    "    (df['dummy_origin'] == 'STRAITS VIEW'),\n",
    "    (df['dummy_origin'] == 'MUSEUM'),\n",
    "    (df['dummy_origin'] == 'ROCHOR'),\n",
    "    (df['dummy_origin'] == 'SINGAPORE RIVER'),\n",
    "    (df['dummy_origin'] == 'RIVER VALLEY'),\n",
    "    (df['dummy_origin'] == 'TANGLIN'),\n",
    "    (df['dummy_origin'] == 'SENGKANG'),\n",
    "    (df['dummy_origin'] == 'SIMPANG'),\n",
    "    (df['dummy_origin'] == 'WESTERN ISLANDS'),\n",
    "    (df['dummy_origin'] == 'WESTERN WATER CATCHMENT')]\n",
    "\n",
    "choices = ['BUKIT PANJANG', 'PAYA LEBAR', 'CLEMENTI', 'MARINA SOUTH','MARINA SOUTH','MARINA SOUTH', 'DOWNTOWN CORE', 'DOWNTOWN CORE','DOWNTOWN CORE',\n",
    "           'ORCHARD', 'ORCHARD', 'SELETAR', 'SEMBAWANG', 'BOON LAY', 'LIM CHU KANG']\n",
    "df['dummy_origin'] = np.select(conditions, choices, df.dummy_origin)\n",
    "\n",
    "# dummy column for destination subregion for joining\n",
    "df['dummy_dest'] = df['dest_subregion']\n",
    "conditions = [\n",
    "    (df['dummy_dest'] == 'BUKIT BATOK'),\n",
    "    (df['dummy_dest'] == 'HOUGANG'),\n",
    "    (df['dummy_dest'] == 'JURONG EAST'),\n",
    "    (df['dummy_dest'] == 'MARINA EAST'),\n",
    "    (df['dummy_dest'] == 'OUTRAM'),\n",
    "    (df['dummy_dest'] == 'STRAITS VIEW'),\n",
    "    (df['dummy_dest'] == 'MUSEUM'),\n",
    "    (df['dummy_dest'] == 'ROCHOR'),\n",
    "    (df['dummy_dest'] == 'SINGAPORE RIVER'),\n",
    "    (df['dummy_dest'] == 'RIVER VALLEY'),\n",
    "    (df['dummy_dest'] == 'TANGLIN'),\n",
    "    (df['dummy_dest'] == 'SENGKANG'),\n",
    "    (df['dummy_dest'] == 'SIMPANG'),\n",
    "    (df['dummy_dest'] == 'WESTERN ISLANDS'),\n",
    "    (df['dummy_dest'] == 'WESTERN WATER CATCHMENT')]\n",
    "\n",
    "choices = ['BUKIT PANJANG', 'PAYA LEBAR', 'CLEMENTI', 'MARINA SOUTH','MARINA SOUTH','MARINA SOUTH', 'DOWNTOWN CORE', 'DOWNTOWN CORE','DOWNTOWN CORE',\n",
    "           'ORCHARD', 'ORCHARD', 'SELETAR', 'SEMBAWANG', 'BOON LAY', 'LIM CHU KANG']\n",
    "df['dummy_dest'] = np.select(conditions, choices, df.dummy_dest)\n",
    "\n",
    "df_a = pd.merge(df, weather,  how='left', left_on=['dummy_origin', 'day','month'], right_on = ['subregion', 'weather_day', 'weather_month'])\n",
    "df_a = df_a.drop(['dummy_origin', 'weather_day', 'weather_month', 'subregion'], axis = 1) \n",
    "df_a.rename(columns={'Rainfall':'origin_avg_daily_rainfall'}, inplace=True)\n",
    "\n",
    "df_b = pd.merge(df_a, weather,  how='left', left_on=['dummy_dest', 'day','month'], right_on = ['subregion', 'weather_day', 'weather_month'])\n",
    "df_b = df_b.drop(['dummy_dest', 'weather_day', 'weather_month', 'subregion'], axis = 1) \n",
    "df_b.rename(columns={'Rainfall':'dest_avg_daily_rainfall'}, inplace=True)\n",
    "\n",
    "df = df_b\n",
    "\n",
    "print('Adding weather data done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27996, 21)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d2obt0_8yzjs"
   },
   "source": [
    "### Final code to putput as .csv file or not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_n3LW4CnyxBE"
   },
   "outputs": [],
   "source": [
    "df_b.to_csv('all_updated_with_weather.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "main.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}